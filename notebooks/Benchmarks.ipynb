{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "In this notebook we will try to identify the potential bottlenecks of the synchronization system, and show a series of benchmarks with different RDF datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we will begin by adding the hercules_sync library to our path, and setting up the logging system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# set up module paths for imports\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "hercules_sync_path = os.path.abspath(os.path.join('..', 'hercules_sync'))\n",
    "sys.path.append(module_path)\n",
    "sys.path.append(hercules_sync_path)\n",
    "\n",
    "# start logging system and set logging level\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)\n",
    "logging.info(\"Starting logger\")\n",
    "\n",
    "%load_ext snakeviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to execute our benchmarks we are going to use two different datasets:\n",
    "* Real dataset: A DBpedia dataset with information about different people is going to be used to represent the performance of our system with a real dataset.\n",
    "* Synthetic dataset: We are also going to use the Berlin SPARQL Benchmark tool to generate datasets of different sizes and observe the performance of our system as the size of the data to be synchronized increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by defining a set of functions that can be used to download the dbpedia dataset and obtain a subset of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import urllib.request\n",
    "\n",
    "def read_zipped_dataset(url, decompressor=bz2):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    content = bz2.decompress(response.read())\n",
    "    return content\n",
    "\n",
    "def get_first_lines(string, num_lines):\n",
    "    return b'\\n'.join(string.split(b'\\n')[:num_lines])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the functions defined above to load the person data from DBpedia. Three different datasets will be stored:\n",
    "* dbpedia_dataset will contain the complete dataset\n",
    "* dbpedia_dataset_preview will contain just the first 100 lines of the dataset. This subset will be used first to identify potential bottlenecks of the system.\n",
    "* dbpedia_dataset_final will contain the first 4000 lines of the dataset. This subset will be used later on for our benchmarks will real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBPEDIA_PERSONDATA_URL = 'http://downloads.dbpedia.org/3.4/en/persondata_en.nt.bz2'\n",
    "NUM_TRIPLES_FINAL = 4000\n",
    "NUM_TRIPLES_PREVIEW = 100\n",
    "\n",
    "dbpedia_dataset = read_zipped_dataset(DBPEDIA_PERSONDATA_URL, bz2)\n",
    "dbpedia_dataset_preview = get_first_lines(dbpedia_dataset, NUM_TRIPLES_PREVIEW)\n",
    "dbpedia_dataset_final = get_first_lines(dbpedia_dataset, NUM_TRIPLES_FINAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will proceed to create the synthetic datasets. We will first create a function that calls the BSBM tool to generate a synthetic dataset with the given number of products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing bottlenecks of the application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to execute an initial dump of the DBpedia preview dataset to identify which are the parts of the synchronization that take the most amount of time to execute. First of all, we are going to define the algorithm to use, and to reset the URIs factory to an initial state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hercules_sync.external.uri_factory_mock import URIFactory\n",
    "from hercules_sync.git import GitFile\n",
    "from hercules_sync.synchronization import GraphDiffSyncAlgorithm, OntologySynchronizer\n",
    "\n",
    "algorithm = GraphDiffSyncAlgorithm()\n",
    "synchronizer = OntologySynchronizer(algorithm)\n",
    "factory = URIFactory()\n",
    "factory.reset_factory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define a function to completely reset the state of the Wikibase instance. Since we are going to make several dumps of data to the Wikibase in this notebook, it is important to reset its state before adding more data. In this case we are connecting to the machine where the Wikibase is running (in docker containers), and calling a script that resets the docker volumes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import time\n",
    "\n",
    "from hercules_sync.triplestore import WikibaseAdapter\n",
    "from secret import SSH_USER, SSH_PASS, USERNAME, PASSWORD\n",
    "\n",
    "wikibase_host = '156.35.94.149'\n",
    "ssh_port = '22'\n",
    "mediawiki_api_url = f'http://{wikibase_host}:8181/w/api.php'\n",
    "sparql_endpoint_url = f'http://{wikibase_host}:8282/proxy/wdqs/bigdata/namespace/wdq/sparql'\n",
    "\n",
    "def reset_wb_state(factory):\n",
    "    print(\"Resetting Wikibase state...\")\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(wikibase_host, ssh_port, SSH_USER, SSH_PASS)\n",
    "    stdin, stdout, stderr = ssh.exec_command('cd ~/wikibase-docker && sh clean_restart.sh')\n",
    "    exit_status = stdout.channel.recv_exit_status()\n",
    "    ssh.close()\n",
    "    factory.reset_factory()\n",
    "    time.sleep(15) # wait some time for docker containers to go up again\n",
    "    print(\"Wikibase state has been reset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the function to be benchmarked. This function executes the synchronization of the given data to Wikibase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_synchronization(source_content, target_content, synchronizer, adapter):\n",
    "    gitfile = GitFile(None, source_content, target_content)\n",
    "    ops = synchronizer.synchronize(gitfile)\n",
    "    for op in ops:\n",
    "        res = op.execute(adapter)\n",
    "        if not res.successful:\n",
    "            print(f\"Error synchronizing triple: {res.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will run the function and obtain some visualizations about its performance using the snakeviz tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://156.35.94.149:8181/w/api.php\n",
      "Successfully logged in as WikibaseAdmin\n",
      " \n",
      "*** Profile stats marshalled to file '/var/folders/p8/9qm2d_ps5rsfbfggf2yrrphm0000gn/T/tmptw5fuhx6'. \n",
      "Embedding SnakeViz in this document...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe id='snakeviz-c58d92be-a8c1-11ea-b92b-804a145cabfb' frameborder=0 seamless width='100%' height='1000'></iframe>\n",
       "<script>document.getElementById(\"snakeviz-c58d92be-a8c1-11ea-b92b-804a145cabfb\").setAttribute(\"src\", \"http://\" + document.location.hostname + \":8080/snakeviz/%2Fvar%2Ffolders%2Fp8%2F9qm2d_ps5rsfbfggf2yrrphm0000gn%2FT%2Ftmptw5fuhx6\")</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset_wb_state(factory)\n",
    "adapter = WikibaseAdapter(mediawiki_api_url, sparql_endpoint_url, USERNAME, PASSWORD)\n",
    "%snakeviz execute_synchronization(\"\", dbpedia_dataset_preview, synchronizer, adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, more than half of the time regarding synchronization is spent in the write method of wdi_core. In the following sections we will try to propose some solutions to alleviate this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch vs Basic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As have seen in the previous section, the writing of triples to Wikibase is the main bottleneck of the system. In order to alleviate this problem, the 'optimize_ops' function is provided to convert a list of BasicOperations into BatchOperations.\n",
    "\n",
    "With this optimization the system will try to perform less writting operations, using instead the 'update' methods provided by WikidataIntegrator. In this section we are going to compare the performance using the basic operations against the 'optimized' version. First of all, we are going to define the functions needed to execute both types of synchronization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _synchronize(source_content, target_content, synchronizer, adapter, ops_callback):\n",
    "    gitfile = GitFile(None, source_content, target_content)\n",
    "    ops = ops_callback(gitfile)\n",
    "    for op in ops:\n",
    "        res = op.execute(adapter)\n",
    "        if not res.successful:\n",
    "            print(f\"Error synchronizing triple: {res.message}\")\n",
    "\n",
    "def execute_basic_synchronization(source_content, target_content, synchronizer, adapter):\n",
    "    ops_callback = lambda f: synchronizer.synchronize(f)\n",
    "    return _synchronize(source_content, target_content, synchronizer, adapter, ops_callback)\n",
    "    \n",
    "def execute_batch_synchronization(source_content, target_content, synchronizer, adapter):\n",
    "    ops_callback = lambda f: optimize_ops(synchronizer.synchronize(f))\n",
    "    return _synchronize(source_content, target_content, synchronizer, adapter, ops_callback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first auxiliary function, '\\_synchronize', is very similar to the synchronization function defined previously to profile the preview dataset. However, in this case we are receiving an additional callback to obtain the list of operations from the file.\n",
    "\n",
    "After that we define two more functions. The first one corresponds to the basic approach used so far, where the operations are obtained directly from our OntologySynchronizer instance. The second function will additionally call the _optimize\\_ops_ function on the operations to optimize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
